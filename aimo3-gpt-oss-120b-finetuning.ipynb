{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"sourceType":"competition"},{"sourceId":289055161,"sourceType":"kernelVersion"},{"sourceId":510391,"sourceType":"modelInstanceVersion","modelInstanceId":404485,"modelId":422384}],"dockerImageVersionId":31236,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### This notebook is based on https://www.kaggle.com/code/andreasbis/aimo-3-gpt-oss-120b-with-tools?scriptVersionId=290379281 (Version 8 with LB 41)\n\nThe following shows the high scoring results of this notebook:\n\n- Version 2 is an exact copy of source but reran a second time (LB 41).\n- Version 2 was submitted again several days later without rerunning and got same score (LB 41); this may be a coincidence but I found it to be unexpected.\n- Version 17 has revised prompts and minor finetuning of a few hyperparameters, including the reduction of Temperature from 1.0 to 0.95.\n\nVersions 18 and 19 are identical to Version 17 except for added acknowledgment comments.\n\nVersion 20 reduced base_problem_timeout from 300 to 270.\n\nps. Version 11 of source notebook reduced base_problem_timeout from 300 to 270 and got improved score of LB 42.\n","metadata":{}},{"cell_type":"code","source":"%pip uninstall --yes 'keras' 'matplotlib' 'scikit-learn' 'tensorflow'","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:32:34.535351Z","iopub.execute_input":"2026-02-04T20:32:34.535637Z","iopub.status.idle":"2026-02-04T20:32:34.544974Z","shell.execute_reply.started":"2026-02-04T20:32:34.535609Z","shell.execute_reply":"2026-02-04T20:32:34.543817Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport sys\nimport subprocess","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:32:45.028550Z","iopub.execute_input":"2026-02-04T20:32:45.028870Z","iopub.status.idle":"2026-02-04T20:32:45.033809Z","shell.execute_reply.started":"2026-02-04T20:32:45.028845Z","shell.execute_reply":"2026-02-04T20:32:45.032625Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def set_env(input_archive, temp_dir):\n\n    if not os.path.exists(temp_dir):\n        os.makedirs(temp_dir, exist_ok=True)\n        \n        subprocess.run(['tar', '-xzf', input_archive, '-C', temp_dir], check=True)\n    \n    subprocess.run([\n        sys.executable, \n        '-m', \n        'pip', \n        'install', \n        '--no-index', \n        '--find-links', \n        f'{temp_dir}/wheels', \n        'unsloth', \n        'trl', \n        'vllm', \n        'openai_harmony'\n    ], check=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:34:17.946225Z","iopub.execute_input":"2026-02-04T20:34:17.946579Z","iopub.status.idle":"2026-02-04T20:34:17.952557Z","shell.execute_reply.started":"2026-02-04T20:34:17.946549Z","shell.execute_reply":"2026-02-04T20:34:17.951603Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"set_env(\n    input_archive='/kaggle/input/aimo-3-utils/wheels.tar.gz', \n    temp_dir='/kaggle/tmp/setup'\n)","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2026-02-04T20:34:19.915060Z","iopub.execute_input":"2026-02-04T20:34:19.915430Z","iopub.status.idle":"2026-02-04T20:40:05.390374Z","shell.execute_reply.started":"2026-02-04T20:34:19.915392Z","shell.execute_reply":"2026-02-04T20:40:05.386906Z"}},"outputs":[{"name":"stdout","text":"Looking in links: /kaggle/tmp/setup/wheels\nProcessing /kaggle/tmp/setup/wheels/unsloth-2025.12.9-py3-none-any.whl\nProcessing /kaggle/tmp/setup/wheels/trl-0.24.0-py3-none-any.whl\nProcessing /kaggle/tmp/setup/wheels/vllm-0.11.2-cp38-abi3-manylinux1_x86_64.whl\nProcessing /kaggle/tmp/setup/wheels/openai_harmony-0.0.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/tmp/setup/wheels/unsloth_zoo-2025.12.7-py3-none-any.whl (from unsloth)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (25.0)\nRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.8.0+cu126)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.23.0+cu126)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\nProcessing /kaggle/tmp/setup/wheels/tyro-1.0.3-py3-none-any.whl (from unsloth)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\nProcessing /kaggle/tmp/setup/wheels/xformers-0.0.33.post1-cp39-abi3-manylinux_2_28_x86_64.whl (from unsloth)\nProcessing /kaggle/tmp/setup/wheels/bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (from unsloth)\nRequirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.4.0)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\nProcessing /kaggle/tmp/setup/wheels/datasets-4.3.0-py3-none-any.whl (from unsloth)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.11.0)\nRequirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.17.1)\nRequirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.36.0)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.2)\nRequirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.3,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.57.1)\nRequirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from vllm) (2025.11.3)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from vllm) (5.5.2)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.32.5)\nRequirement already satisfied: blake3 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.0.8)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm) (9.0.0)\nRequirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.22.1)\nRequirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.119.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from vllm) (3.13.2)\nRequirement already satisfied: openai>=1.99.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.109.1)\nRequirement already satisfied: pydantic>=2.12.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.12.5)\nRequirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.23.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from vllm) (11.3.0)\nProcessing /kaggle/tmp/setup/wheels/prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (from vllm)\nRequirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.12.0)\nProcessing /kaggle/tmp/setup/wheels/lm_format_enforcer-0.11.3-py3-none-any.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/llguidance-1.3.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/diskcache-5.6.3-py3-none-any.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/lark-1.2.2-py3-none-any.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/xgrammar-0.1.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\nRequirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.15.0)\nRequirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (3.20.1)\nProcessing /kaggle/tmp/setup/wheels/partial_json_parser-0.2.1.1.post7-py3-none-any.whl (from vllm)\nRequirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (26.2.1)\nProcessing /kaggle/tmp/setup/wheels/msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/gguf-0.17.1-py3-none-any.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/mistral_common-1.8.8-py3-none-any.whl (from mistral_common[image]>=1.8.5->vllm)\nRequirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.12.0.88)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from vllm) (6.0.3)\nRequirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.17.0)\nProcessing /kaggle/tmp/setup/wheels/setuptools-80.9.0-py3-none-any.whl (from vllm)\nRequirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm) (0.8.1)\nProcessing /kaggle/tmp/setup/wheels/compressed_tensors-0.12.2-py3-none-any.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/depyf-0.20.0-py3-none-any.whl (from vllm)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from vllm) (3.1.1)\nProcessing /kaggle/tmp/setup/wheels/watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\nRequirement already satisfied: python-json-logger in /usr/local/lib/python3.12/dist-packages (from vllm) (4.0.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from vllm) (1.15.3)\nRequirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from vllm) (1.13.0)\nProcessing /kaggle/tmp/setup/wheels/pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/cbor2-5.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/anthropic-0.71.0-py3-none-any.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/model_hosting_container_standards-0.1.12-py3-none-any.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from vllm)\nRequirement already satisfied: ray>=2.48.0 in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]>=2.48.0->vllm) (2.52.1)\nProcessing /kaggle/tmp/setup/wheels/torch-2.9.0+cu128-cp312-cp312-manylinux_2_28_x86_64.whl (from unsloth)\nProcessing /kaggle/tmp/setup/wheels/torchaudio-2.9.0+cu128-cp312-cp312-manylinux_2_28_x86_64.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/torchvision-0.24.0+cu128-cp312-cp312-manylinux_2_28_x86_64.whl (from unsloth)\nProcessing /kaggle/tmp/setup/wheels/flashinfer_python-0.5.2-py3-none-any.whl (from vllm)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (4.12.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (1.9.0)\nRequirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (0.17.0)\nRequirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (0.11.1)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (1.3.1)\nProcessing /kaggle/tmp/setup/wheels/loguru-0.7.3-py3-none-any.whl (from compressed-tensors==0.12.2->vllm)\nProcessing /kaggle/tmp/setup/wheels/astor-0.8.1-py2.py3-none-any.whl (from depyf==0.20.0->vllm)\nRequirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from depyf==0.20.0->vllm) (0.4.0)\nProcessing /kaggle/tmp/setup/wheels/apache_tvm_ffi-0.1.7-cp312-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (from flashinfer-python==0.5.2->vllm)\nRequirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from flashinfer-python==0.5.2->vllm) (8.3.1)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cudnn_frontend-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (from flashinfer-python==0.5.2->vllm)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cutlass_dsl-4.3.4-cp312-cp312-manylinux_2_28_x86_64.whl (from flashinfer-python==0.5.2->vllm)\nRequirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.12/dist-packages (from flashinfer-python==0.5.2->vllm) (12.575.51)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from flashinfer-python==0.5.2->vllm) (0.9.0)\nProcessing /kaggle/tmp/setup/wheels/interegular-0.3.3-py37-none-any.whl (from lm-format-enforcer==0.11.3->vllm)\nProcessing /kaggle/tmp/setup/wheels/llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from numba==0.61.2->vllm)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.13.3)\nRequirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\nRequirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2025.10.0)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\nProcessing /kaggle/tmp/setup/wheels/nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\nProcessing /kaggle/tmp/setup/wheels/triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (from unsloth)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (0.6.2)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (22.0.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.2.2)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.6.0)\nProcessing /kaggle/tmp/setup/wheels/multiprocess-0.70.16-py312-none-any.whl (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth)\nProcessing /kaggle/tmp/setup/wheels/fsspec-2025.9.0-py3-none-any.whl (from torch>=2.4.0->unsloth)\nRequirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.48.0)\nProcessing /kaggle/tmp/setup/wheels/fastapi_cli-0.0.20-py3-none-any.whl (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nRequirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\nRequirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.3.0)\nRequirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.38.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.2.1rc0)\nRequirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (4.25.1)\nProcessing /kaggle/tmp/setup/wheels/pydantic_extra_types-2.10.6-py3-none-any.whl (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm)\nRequirement already satisfied: jmespath in /usr/local/lib/python3.12/dist-packages (from model-hosting-container-standards<1.0.0->vllm) (1.0.1)\nINFO: pip is looking at multiple versions of model-hosting-container-standards to determine which version is compatible with other requirements. This could take a while.\nProcessing /kaggle/tmp/setup/wheels/fastapi-0.128.0-py3-none-any.whl (from vllm)\nProcessing /kaggle/tmp/setup/wheels/annotated_doc-0.0.4-py3-none-any.whl (from fastapi[standard]>=0.115.0->vllm)\nRequirement already satisfied: pydantic-settings>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.11.0)\nProcessing /kaggle/tmp/setup/wheels/starlette-0.50.0-py3-none-any.whl (from fastapi[standard]>=0.115.0->vllm)\nProcessing /kaggle/tmp/setup/wheels/supervisor-4.3.0-py2.py3-none-any.whl (from model-hosting-container-standards<1.0.0->vllm)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.12.0->vllm) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.12.0->vllm) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.12.0->vllm) (0.4.2)\nINFO: pip is looking at multiple versions of ray to determine which version is compatible with other requirements. This could take a while.\nProcessing /kaggle/tmp/setup/wheels/ray-2.53.0-cp312-cp312-manylinux2014_x86_64.whl (from ray[cgraph]>=2.48.0->vllm)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (1.1.2)\nINFO: pip is looking at multiple versions of ray[cgraph] to determine which version is compatible with other requirements. This could take a while.\nRequirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]>=2.48.0->vllm) (13.3.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (2025.11.12)\nProcessing /kaggle/tmp/setup/wheels/torchao-0.15.0+cu128-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (from unsloth_zoo>=2025.12.7->unsloth)\nProcessing /kaggle/tmp/setup/wheels/cut_cross_entropy-25.1.1-py3-none-any.whl (from unsloth_zoo>=2025.12.7->unsloth)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.22.0)\nRequirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.0)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\nRequirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.8.0)\nRequirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.20.0)\nProcessing /kaggle/tmp/setup/wheels/rich_toolkit-0.17.1-py3-none-any.whl (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nProcessing /kaggle/tmp/setup/wheels/fastapi_cloud_cli-0.8.0-py3-none-any.whl (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.3)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (2025.9.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (0.37.0)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (0.27.1)\nProcessing /kaggle/tmp/setup/wheels/cuda_python-13.1.1-py3-none-any.whl (from nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.2->vllm)\nProcessing /kaggle/tmp/setup/wheels/pycountry-24.6.1-py3-none-any.whl (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.0.0->fastapi[standard]>=0.115.0->vllm) (1.1.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\nProcessing /kaggle/tmp/setup/wheels/httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nProcessing /kaggle/tmp/setup/wheels/uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\nRequirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm) (0.8.3)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.3)\nProcessing /kaggle/tmp/setup/wheels/cuda_bindings-13.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (from cuda-python>=12.8->nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.2->vllm)\nProcessing /kaggle/tmp/setup/wheels/cuda_pathfinder-1.3.3-py3-none-any.whl (from cuda-python>=12.8->nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.2->vllm)\nProcessing /kaggle/tmp/setup/wheels/rignore-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nRequirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.42.1)\nProcessing /kaggle/tmp/setup/wheels/fastar-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nRequirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.2.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\nInstalling collected packages: torchao, supervisor, uvloop, triton, setuptools, setproctitle, rignore, pycountry, pybase64, partial-json-parser, outlines_core, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cudnn-frontend, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multiprocess, msgspec, loguru, llvmlite, llguidance, lark, interegular, httptools, gguf, fsspec, fastar, diskcache, cuda-pathfinder, cbor2, astor, apache-tvm-ffi, annotated-doc, watchfiles, tyro, starlette, nvidia-cusparse-cu12, nvidia-cufft-cu12, numba, depyf, cuda-bindings, rich-toolkit, pydantic-extra-types, prometheus-fastapi-instrumentator, openai_harmony, nvidia-cusolver-cu12, lm-format-enforcer, fastapi, cuda-python, anthropic, torch, ray, nvidia-cutlass-dsl, model-hosting-container-standards, fastapi-cloud-cli, fastapi-cli, datasets, xgrammar, xformers, torchvision, torchaudio, mistral_common, flashinfer-python, cut_cross_entropy, compressed-tensors, bitsandbytes, trl, unsloth_zoo, vllm, unsloth\n  Attempting uninstall: torchao\n    Found existing installation: torchao 0.10.0\n    Uninstalling torchao-0.10.0:\n      Successfully uninstalled torchao-0.10.0\n  Attempting uninstall: triton\n    Found existing installation: triton 3.4.0\n    Uninstalling triton-3.4.0:\n      Successfully uninstalled triton-3.4.0\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 75.2.0\n    Uninstalling setuptools-75.2.0:\n      Successfully uninstalled setuptools-75.2.0\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.6.77\n    Uninstalling nvidia-nvtx-cu12-12.6.77:\n      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n  Attempting uninstall: nvidia-nvshmem-cu12\n    Found existing installation: nvidia-nvshmem-cu12 3.4.5\n    Uninstalling nvidia-nvshmem-cu12-3.4.5:\n      Successfully uninstalled nvidia-nvshmem-cu12-3.4.5\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.27.3\n    Uninstalling nvidia-nccl-cu12-2.27.3:\n      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufile-cu12\n    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.18\n    Uninstalling multiprocess-0.70.18:\n      Successfully uninstalled multiprocess-0.70.18\n  Attempting uninstall: llvmlite\n    Found existing installation: llvmlite 0.43.0\n    Uninstalling llvmlite-0.43.0:\n      Successfully uninstalled llvmlite-0.43.0\n  Attempting uninstall: lark\n    Found existing installation: lark 1.3.0\n    Uninstalling lark-1.3.0:\n      Successfully uninstalled lark-1.3.0\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.10.0\n    Uninstalling fsspec-2025.10.0:\n      Successfully uninstalled fsspec-2025.10.0\n  Attempting uninstall: starlette\n    Found existing installation: starlette 0.48.0\n    Uninstalling starlette-0.48.0:\n      Successfully uninstalled starlette-0.48.0\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: numba\n    Found existing installation: numba 0.60.0\n    Uninstalling numba-0.60.0:\n      Successfully uninstalled numba-0.60.0\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n  Attempting uninstall: fastapi\n    Found existing installation: fastapi 0.119.1\n    Uninstalling fastapi-0.119.1:\n      Successfully uninstalled fastapi-0.119.1\n  Attempting uninstall: cuda-python\n    Found existing installation: cuda-python 12.6.2.post1\n    Uninstalling cuda-python-12.6.2.post1:\n      Successfully uninstalled cuda-python-12.6.2.post1\n  Attempting uninstall: torch\n    Found existing installation: torch 2.8.0+cu126\n    Uninstalling torch-2.8.0+cu126:\n      Successfully uninstalled torch-2.8.0+cu126\n  Attempting uninstall: ray\n    Found existing installation: ray 2.52.1\n    Uninstalling ray-2.52.1:\n      Successfully uninstalled ray-2.52.1\n  Attempting uninstall: datasets\n    Found existing installation: datasets 4.4.1\n    Uninstalling datasets-4.4.1:\n      Successfully uninstalled datasets-4.4.1\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.23.0+cu126\n    Uninstalling torchvision-0.23.0+cu126:\n      Successfully uninstalled torchvision-0.23.0+cu126\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.8.0+cu126\n    Uninstalling torchaudio-2.8.0+cu126:\n      Successfully uninstalled torchaudio-2.8.0+cu126\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.2.19 which is incompatible.\ncudf-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\ncuml-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\npylibraft-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\ncuvs-cu12 25.6.1 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\nfastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.9.0+cu128 which is incompatible.\nrmm-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\npylibcudf-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"Successfully installed annotated-doc-0.0.4 anthropic-0.71.0 apache-tvm-ffi-0.1.7 astor-0.8.1 bitsandbytes-0.49.0 cbor2-5.7.1 compressed-tensors-0.12.2 cuda-bindings-13.1.1 cuda-pathfinder-1.3.3 cuda-python-13.1.1 cut_cross_entropy-25.1.1 datasets-4.3.0 depyf-0.20.0 diskcache-5.6.3 fastapi-0.128.0 fastapi-cli-0.0.20 fastapi-cloud-cli-0.8.0 fastar-0.8.0 flashinfer-python-0.5.2 fsspec-2025.9.0 gguf-0.17.1 httptools-0.7.1 interegular-0.3.3 lark-1.2.2 llguidance-1.3.0 llvmlite-0.44.0 lm-format-enforcer-0.11.3 loguru-0.7.3 mistral_common-1.8.8 model-hosting-container-standards-0.1.12 msgspec-0.20.0 multiprocess-0.70.16 numba-0.61.2 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-frontend-1.17.0 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cutlass-dsl-4.3.4 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 openai_harmony-0.0.8 outlines_core-0.2.11 partial-json-parser-0.2.1.1.post7 prometheus-fastapi-instrumentator-7.1.0 pybase64-1.4.3 pycountry-24.6.1 pydantic-extra-types-2.10.6 ray-2.53.0 rich-toolkit-0.17.1 rignore-0.7.6 setproctitle-1.3.7 setuptools-80.9.0 starlette-0.50.0 supervisor-4.3.0 torch-2.9.0+cu128 torchao-0.15.0+cu128 torchaudio-2.9.0+cu128 torchvision-0.24.0+cu128 triton-3.5.0 trl-0.24.0 tyro-1.0.3 unsloth-2025.12.9 unsloth_zoo-2025.12.7 uvloop-0.22.1 vllm-0.11.2 watchfiles-1.1.1 xformers-0.0.33.post1 xgrammar-0.1.25\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"subprocess.run(['ls', '/kaggle/tmp/setup/tiktoken_encodings'])","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ['TRANSFORMERS_NO_TF'] = '1'\nos.environ['TRANSFORMERS_NO_FLAX'] = '1'\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nos.environ['TRITON_PTXAS_PATH'] = '/usr/local/cuda/bin/ptxas'\nos.environ['TIKTOKEN_ENCODINGS_BASE'] = '/kaggle/tmp/setup/tiktoken_encodings'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:41:23.562940Z","iopub.execute_input":"2026-02-04T20:41:23.564062Z","iopub.status.idle":"2026-02-04T20:41:23.568927Z","shell.execute_reply.started":"2026-02-04T20:41:23.564022Z","shell.execute_reply":"2026-02-04T20:41:23.567913Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import gc\nimport re\nimport math\nimport time\nimport queue\nimport threading\nimport contextlib\nfrom typing import Optional\nfrom jupyter_client import KernelManager\nfrom collections import Counter, defaultdict\nfrom concurrent.futures import as_completed, ThreadPoolExecutor\n\nimport pandas as pd\nimport polars as pl\n\nfrom openai import OpenAI\n\nfrom openai_harmony import (\n    HarmonyEncodingName, \n    load_harmony_encoding, \n    SystemContent, \n    ReasoningEffort, \n    ToolNamespaceConfig, \n    Author, \n    Message, \n    Role, \n    TextContent, \n    Conversation\n)\n\nfrom transformers import set_seed\nimport kaggle_evaluation.aimo_3_inference_server","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:41:27.903128Z","iopub.execute_input":"2026-02-04T20:41:27.903500Z","iopub.status.idle":"2026-02-04T20:41:38.778213Z","shell.execute_reply.started":"2026-02-04T20:41:27.903468Z","shell.execute_reply":"2026-02-04T20:41:38.776857Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class CFG:\n\n    system_prompt = (\n        'You a world-class competitor in a math competition in the style of International Mathematical Olympiad. '\n        'The final answer for each question must be an integer between 0 and 99999. '\n        'The final integer answer must be placed inside \\\\boxed{}.'\n    )\n    \n    tool_prompt = (\n        'Use this tool to execute Python codes in a Jupyter notebook. '\n        'Use print() to output results.'\n    )\n    \n    preference_prompt = (\n        'There is access to `math`, `numpy` and `sympy` and other modules to solve these math problems.'\n    )\n\n    served_model_name = 'gpt-oss'\n    model_path = '/kaggle/input/gpt-oss-120b/transformers/default/1'\n    \n    kv_cache_dtype = 'fp8_e4m3'\n    dtype = 'auto'\n\n    high_problem_timeout = 900\n    base_problem_timeout = 270\n\n    notebook_limit = 17400\n    server_timeout = 180\n\n    session_timeout = 960\n    jupyter_timeout = 6\n    sandbox_timeout = 3\n\n    stream_interval = 200\n    context_tokens = 65536\n    buffer_tokens = 512\n    search_tokens = 32\n    top_logprobs = 5\n    batch_size = 256\n    early_stop = 4\n    attempts = 8\n    workers = 16\n    turns = 128\n    seed = 2570\n\n    gpu_memory_utilization = 0.95\n    temperature = 0.95\n    min_p = 0.02","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:41:46.523186Z","iopub.execute_input":"2026-02-04T20:41:46.523748Z","iopub.status.idle":"2026-02-04T20:41:46.531731Z","shell.execute_reply.started":"2026-02-04T20:41:46.523715Z","shell.execute_reply":"2026-02-04T20:41:46.530365Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"set_seed(CFG.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:41:50.069399Z","iopub.execute_input":"2026-02-04T20:41:50.069812Z","iopub.status.idle":"2026-02-04T20:42:10.647386Z","shell.execute_reply.started":"2026-02-04T20:41:50.069781Z","shell.execute_reply":"2026-02-04T20:42:10.645935Z"}},"outputs":[{"name":"stderr","text":"2026-02-04 20:41:52.220529: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770237712.485288      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770237712.561170      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770237713.122347      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770237713.122399      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770237713.122402      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770237713.122406      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"class AIMO3Template:\n\n    def __init__(self):\n\n        pass\n\n    def get_system_content(self, system_prompt: str, tool_config: ToolNamespaceConfig) -> SystemContent:\n\n        return (\n            SystemContent.new()\n            .with_model_identity(system_prompt)\n            .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)\n            .with_tools(tool_config)\n        )\n\n    def apply_chat_template(\n        self, \n        system_prompt: str, \n        user_prompt: str, \n        tool_config: ToolNamespaceConfig\n    ) -> list[Message]:\n\n        system_content = self.get_system_content(system_prompt, tool_config)        \n        system_message = Message.from_role_and_content(Role.SYSTEM, system_content)\n\n        user_message = Message.from_role_and_content(Role.USER, user_prompt)\n\n        return [system_message, user_message]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:42:30.592440Z","iopub.execute_input":"2026-02-04T20:42:30.594028Z","iopub.status.idle":"2026-02-04T20:42:30.601583Z","shell.execute_reply.started":"2026-02-04T20:42:30.593979Z","shell.execute_reply":"2026-02-04T20:42:30.600210Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class AIMO3Sandbox:\n\n    _port_lock = threading.Lock()\n    _next_port = 50000\n\n    @classmethod\n    def _get_next_ports(cls, count: int = 5) -> list[int]:\n\n        with cls._port_lock:\n            ports = list(range(cls._next_port, cls._next_port + count))\n            cls._next_port += count\n\n            return ports\n\n    def __init__(self, timeout: float):\n\n        self._default_timeout = timeout\n        self._owns_kernel = False\n        self._client = None\n        self._km = None\n        \n        ports = self._get_next_ports(5)\n\n        env = os.environ.copy()\n        env['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n        env['PYDEVD_WARN_EVALUATION_TIMEOUT'] = '0'\n        env['JUPYTER_PLATFORM_DIRS'] = '1'\n        env['PYTHONWARNINGS'] = 'ignore'\n        env['MPLBACKEND'] = 'Agg'\n\n        self._km = KernelManager()\n        self._km.shell_port = ports[0]\n        self._km.iopub_port = ports[1]\n        self._km.stdin_port = ports[2]\n        self._km.hb_port = ports[3]\n        self._km.control_port = ports[4]\n\n        self._km.start_kernel(env=env, extra_arguments=['--Application.log_level=CRITICAL'])\n\n        self._client = self._km.blocking_client()\n        self._client.start_channels()\n        self._client.wait_for_ready(timeout=self._default_timeout)\n        self._owns_kernel = True\n\n        self.execute(\n            'import math\\n'\n            'import numpy\\n'\n            'import sympy\\n'\n            'import itertools\\n'\n            'import collections\\n'\n            'import mpmath\\n'\n            'mpmath.mp.dps = 64\\n'\n        )\n\n    def _format_error(self, traceback: list[str]) -> str:\n\n        clean_lines = []\n\n        for frame in traceback:\n            clean_frame = re.sub(r'\\x1b\\[[0-9;]*m', '', frame)\n\n            if 'File \"' in clean_frame and 'ipython-input' not in clean_frame:\n                continue\n\n            clean_lines.append(clean_frame)\n\n        return ''.join(clean_lines)\n\n    def execute(self, code: str, timeout: float | None = None) -> str:\n\n        client = self._client\n        effective_timeout = timeout or self._default_timeout\n        \n        msg_id = client.execute(\n            code, \n            store_history=True, \n            allow_stdin=False, \n            stop_on_error=False\n        )\n\n        stdout_parts = []\n        stderr_parts = []\n        \n        start_time = time.time()\n\n        while True:\n            elapsed = time.time() - start_time\n\n            if elapsed > effective_timeout:\n                self._km.interrupt_kernel()\n\n                return f'[ERROR] Execution timed out after {effective_timeout} seconds'\n\n            try:\n                msg = client.get_iopub_msg(timeout=1.0)\n\n            except queue.Empty:\n                continue\n\n            if msg.get('parent_header', {}).get('msg_id') != msg_id:\n                continue\n\n            msg_type = msg.get('msg_type')\n            content = msg.get('content', {})\n\n            if msg_type == 'stream':\n                text = content.get('text', '')\n\n                if content.get('name') == 'stdout':\n                    stdout_parts.append(text)\n\n                else:\n                    stderr_parts.append(text)\n\n            elif msg_type == 'error':\n                traceback_list = content.get('traceback', [])\n\n                stderr_parts.append(self._format_error(traceback_list))\n\n            elif msg_type in {'execute_result', 'display_data'}:\n                data = content.get('data', {})\n                text = data.get('text/plain')\n\n                if text:\n                    stdout_parts.append(text if text.endswith('\\n') else f'{text}\\n')\n\n            elif msg_type == 'status':\n                if content.get('execution_state') == 'idle':\n                    break\n\n        stdout = ''.join(stdout_parts)\n        stderr = ''.join(stderr_parts)\n\n        if stderr:\n            return f'{stdout.rstrip()}\\n{stderr}' if stdout else stderr\n\n        return stdout if stdout.strip() else '[WARN] No output. Use print() to see results.'\n\n    def close(self):\n\n        with contextlib.suppress(Exception):\n            if self._client:\n                self._client.stop_channels()\n\n        if self._owns_kernel and self._km is not None:\n            with contextlib.suppress(Exception):\n                self._km.shutdown_kernel(now=True)\n\n            with contextlib.suppress(Exception):\n                self._km.cleanup_resources()\n\n    def reset(self):\n        \n        self.execute(\n            '%reset -f\\n'\n            'import math\\n'\n            'import numpy\\n'\n            'import sympy\\n'\n            'import itertools\\n'\n            'import collections\\n'\n            'import mpmath\\n'\n            'mpmath.mp.dps = 64\\n'\n        )\n\n    def __del__(self):\n\n        self.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:42:40.902000Z","iopub.execute_input":"2026-02-04T20:42:40.902409Z","iopub.status.idle":"2026-02-04T20:42:40.924833Z","shell.execute_reply.started":"2026-02-04T20:42:40.902379Z","shell.execute_reply":"2026-02-04T20:42:40.923640Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class AIMO3Tool:\n\n    def __init__(self, local_jupyter_timeout: float, tool_prompt: str, sandbox=None):\n\n        self._local_jupyter_timeout = local_jupyter_timeout\n        self._tool_prompt = tool_prompt\n        self._jupyter_session = sandbox\n        \n        self._owns_session = sandbox is None\n        \n        self._execution_lock = threading.Lock()\n        self._init_lock = threading.Lock()\n\n    def _ensure_session(self):\n\n        if self._jupyter_session is None:\n            with self._init_lock:\n                if self._jupyter_session is None:\n                    self._jupyter_session = AIMO3Sandbox(timeout=self._local_jupyter_timeout)\n\n    def _ensure_last_print(self, code: str) -> str:\n\n        lines = code.strip().split('\\n')\n\n        if not lines:\n            return code\n\n        last_line = lines[-1].strip()\n\n        if 'print' in last_line or 'import' in last_line:\n            return code\n\n        if not last_line:\n            return code\n\n        if last_line.startswith('#'):\n            return code\n\n        lines[-1] = 'print(' + last_line + ')'\n\n        return '\\n'.join(lines)\n\n    @property\n    def instruction(self) -> str:\n\n        return self._tool_prompt\n\n    @property\n    def tool_config(self) -> ToolNamespaceConfig:\n\n        return ToolNamespaceConfig(\n            name='python', \n            description=self.instruction, \n            tools=[]\n        )\n\n    def _make_response(self, output: str, channel: str | None = None) -> Message:\n\n        content = TextContent(text=output)\n        author = Author(role=Role.TOOL, name='python')\n        message = Message(author=author, content=[content]).with_recipient('assistant')\n\n        if channel:\n            message = message.with_channel(channel)\n\n        return message\n\n    def process_sync_plus(self, message: Message) -> list[Message]:\n\n        self._ensure_session()\n        raw_script = message.content[0].text\n        final_script = self._ensure_last_print(raw_script)\n\n        with self._execution_lock:\n            try:\n                output = self._jupyter_session.execute(final_script)\n\n            except TimeoutError as exc:\n                output = f'[ERROR] {exc}'\n\n        return [self._make_response(output, channel=message.channel)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:42:44.305221Z","iopub.execute_input":"2026-02-04T20:42:44.305608Z","iopub.status.idle":"2026-02-04T20:42:44.317544Z","shell.execute_reply.started":"2026-02-04T20:42:44.305577Z","shell.execute_reply":"2026-02-04T20:42:44.316561Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class AIMO3Solver:\n\n    def __init__(self, cfg, port: int = 8000):\n    \n        self.cfg = cfg\n        self.port = port\n        self.base_url = f'http://0.0.0.0:{port}/v1'\n        self.api_key = 'sk-local'\n        self.template = AIMO3Template()\n        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n    \n        self._preload_model_weights()\n        \n        self.server_process = self._start_server()\n    \n        self.client = OpenAI(\n            base_url=self.base_url, \n            api_key=self.api_key, \n            timeout=self.cfg.session_timeout\n        )\n    \n        self._wait_for_server()\n        self._initialize_kernels()\n    \n        self.notebook_start_time = time.time()\n        self.problems_remaining = 50\n    \n    def _preload_model_weights(self) -> None:\n    \n        print(f'Loading model weights from {self.cfg.model_path} into OS Page Cache...')\n        start_time = time.time()\n        \n        files_to_load = []\n        total_size = 0\n    \n        for root, _, files in os.walk(self.cfg.model_path):\n            for file_name in files:\n                file_path = os.path.join(root, file_name)\n    \n                if os.path.isfile(file_path):\n                    files_to_load.append(file_path)\n                    total_size += os.path.getsize(file_path)\n    \n        def _read_file(path: str) -> None:\n    \n            with open(path, 'rb') as file_object:\n                while file_object.read(1024 * 1024 * 1024):\n                    pass\n    \n        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n            list(executor.map(_read_file, files_to_load))\n    \n        elapsed = time.time() - start_time\n        print(f'Processed {len(files_to_load)} files ({total_size / 1e9:.2f} GB) in {elapsed:.2f} seconds.\\n')\n    \n    def _start_server(self) -> subprocess.Popen:\n    \n        cmd = [\n            sys.executable, \n            '-m', \n            'vllm.entrypoints.openai.api_server', \n            '--seed', \n            str(self.cfg.seed), \n            '--model', \n            self.cfg.model_path, \n            '--served-model-name', \n            self.cfg.served_model_name, \n            '--tensor-parallel-size', \n            '1', \n            '--max-num-seqs', \n            str(self.cfg.batch_size), \n            '--gpu-memory-utilization', \n            str(self.cfg.gpu_memory_utilization), \n            '--host', \n            '0.0.0.0', \n            '--port', \n            str(self.port), \n            '--dtype', \n            self.cfg.dtype, \n            '--kv-cache-dtype', \n            self.cfg.kv_cache_dtype, \n            '--max-model-len', \n            str(self.cfg.context_tokens), \n            '--stream-interval', \n            str(self.cfg.stream_interval), \n            '--async-scheduling', \n            '--disable-log-stats', \n            '--enable-prefix-caching'\n        ]\n    \n        self.log_file = open('vllm_server.log', 'w')\n    \n        return subprocess.Popen(\n            cmd, \n            stdout=self.log_file, \n            stderr=subprocess.STDOUT, \n            start_new_session=True\n        )\n    \n    def _wait_for_server(self):\n    \n        print('Waiting for vLLM server...')\n        start_time = time.time()\n    \n        for _ in range(self.cfg.server_timeout):\n            return_code = self.server_process.poll()\n    \n            if return_code is not None:\n                self.log_file.flush()\n    \n                with open('vllm_server.log', 'r') as log_file:\n                    logs = log_file.read()\n    \n                raise RuntimeError(f'Server died with code {return_code}. Full logs:\\n{logs}\\n')\n    \n            try:\n                self.client.models.list()\n                elapsed = time.time() - start_time\n                print(f'Server is ready (took {elapsed:.2f} seconds).\\n')\n    \n                return\n    \n            except Exception:\n                time.sleep(1)\n    \n        raise RuntimeError('Server failed to start (timeout).\\n')\n    \n    def _initialize_kernels(self) -> None:\n    \n        print(f'Initializing {self.cfg.workers} persistent Jupyter kernels...')\n        start_time = time.time()\n    \n        self.sandbox_pool = queue.Queue()\n    \n        def _create_sandbox():\n            \n            return AIMO3Sandbox(timeout=self.cfg.jupyter_timeout)\n    \n        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n            futures = [executor.submit(_create_sandbox) for _ in range(self.cfg.workers)]\n    \n            for future in as_completed(futures):\n                self.sandbox_pool.put(future.result())\n    \n        elapsed = time.time() - start_time\n        print(f'Kernels initialized in {elapsed:.2f} seconds.\\n')\n    \n    def _scan_for_answer(self, text: str) -> int | None:\n        \n        pattern = r'\\\\boxed\\s*\\{\\s*([0-9,]+)\\s*\\}'\n        matches = re.findall(pattern, text)\n    \n        if matches:\n            try:\n                clean_value = matches[-1].replace(',', '')\n                value = int(clean_value)\n    \n                if 0 <= value <= 99999:\n                    return value\n    \n            except ValueError:\n                pass\n                \n        pattern = r'final\\s+answer\\s+is\\s*([0-9,]+)'\n        matches = re.findall(pattern, text, re.IGNORECASE)\n    \n        if matches:\n            try:\n                clean_value = matches[-1].replace(',', '')\n                value = int(clean_value)\n    \n                if 0 <= value <= 99999:\n                    return value\n    \n            except ValueError:\n                pass\n    \n        return None\n    \n    def _compute_mean_entropy(self, logprobs_buffer: list) -> float:\n    \n        if not logprobs_buffer:\n            return float('inf')\n    \n        total_entropy = 0.0\n        token_count = 0\n    \n        for top_logprobs_dict in logprobs_buffer:\n            \n            if not isinstance(top_logprobs_dict, dict):\n                continue\n            \n            if not top_logprobs_dict:\n                continue\n            \n            token_entropy = 0.0\n            \n            for token_str, log_prob in top_logprobs_dict.items():\n                prob = math.exp(log_prob)\n                \n                if prob > 0:\n                    token_entropy -= prob * math.log2(prob)\n            \n            total_entropy += token_entropy\n            token_count += 1\n    \n        if token_count == 0:\n            return float('inf')\n    \n        return total_entropy / token_count\n    \n    def _process_attempt(\n        self, \n        problem: str, \n        system_prompt: str, \n        attempt_index: int, \n        stop_event: threading.Event, \n        deadline: float\n    ) -> dict:\n    \n        if stop_event.is_set() or time.time() > deadline:\n            return {\n                'Attempt': attempt_index + 1, \n                'Answer': None, \n                'Python Calls': 0, \n                'Python Errors': 0, \n                'Response Length': 0, \n                'Entropy': float('inf')\n            }\n    \n        local_tool = None\n        sandbox = None\n        python_calls = 0\n        python_errors = 0\n        total_tokens = 0\n        final_answer = None\n        \n        logprobs_buffer = []\n    \n        attempt_seed = int(math.pow(self.cfg.seed + attempt_index, 2))\n    \n        try:\n            sandbox = self.sandbox_pool.get(timeout=self.cfg.sandbox_timeout)\n    \n            local_tool = AIMO3Tool(\n                local_jupyter_timeout=self.cfg.jupyter_timeout, \n                tool_prompt=self.cfg.tool_prompt, \n                sandbox=sandbox\n            )\n    \n            encoding = self.encoding\n            messages = self.template.apply_chat_template(\n                system_prompt, \n                problem, \n                local_tool.tool_config\n            )\n    \n            conversation = Conversation.from_messages(messages)\n    \n            for _ in range(self.cfg.turns):\n                if stop_event.is_set() or time.time() > deadline:\n                    break\n    \n                prompt_ids = encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n                max_tokens = self.cfg.context_tokens - len(prompt_ids)\n    \n                if max_tokens < self.cfg.buffer_tokens:\n                    break\n    \n                stream = self.client.completions.create(\n                    model=self.cfg.served_model_name, \n                    temperature=self.cfg.temperature, \n                    logprobs=self.cfg.top_logprobs, \n                    max_tokens=max_tokens, \n                    prompt=prompt_ids, \n                    seed=attempt_seed, \n                    stream=True, \n                    extra_body={\n                        'min_p': self.cfg.min_p, \n                        'stop_token_ids': self.stop_token_ids, \n                        'return_token_ids': True\n                    }\n                )\n    \n                try:\n                    token_buffer = []\n                    text_chunks = []\n    \n                    for chunk in stream:\n                        if stop_event.is_set() or time.time() > deadline:\n                            break\n    \n                        new_tokens = chunk.choices[0].token_ids\n                        new_text = chunk.choices[0].text\n    \n                        if new_tokens:\n                            token_buffer.extend(new_tokens)\n                            total_tokens += len(new_tokens)\n                            text_chunks.append(new_text)\n                            \n                            chunk_logprobs = chunk.choices[0].logprobs\n                            \n                            if chunk_logprobs is not None:\n                                if chunk_logprobs.top_logprobs:\n                                    logprobs_buffer.extend(chunk_logprobs.top_logprobs)\n    \n                        if '}' in new_text:\n                            search_text = ''.join(text_chunks[-self.cfg.search_tokens:])\n                            answer = self._scan_for_answer(search_text)\n    \n                            if answer is not None:\n                                final_answer = answer\n                                break\n    \n                finally:\n                    stream.close()\n    \n                if final_answer is not None:\n                    break\n    \n                if not token_buffer:\n                    break\n    \n                new_messages = encoding.parse_messages_from_completion_tokens(token_buffer, Role.ASSISTANT)\n                conversation.messages.extend(new_messages)\n                last_message = new_messages[-1]\n    \n                if last_message.channel == 'final':\n                    answer_text = last_message.content[0].text\n                    final_answer = self._scan_for_answer(answer_text)\n                    break\n    \n                if last_message.recipient == 'python':\n                    python_calls += 1\n                    tool_responses = local_tool.process_sync_plus(last_message)\n    \n                    response_text = tool_responses[0].content[0].text\n    \n                    if response_text.startswith('[ERROR]') or 'Traceback' in response_text or 'Error:' in response_text:\n                        python_errors += 1\n    \n                    conversation.messages.extend(tool_responses)\n    \n        except Exception as exc:\n            python_errors += 1\n    \n        finally:\n            if sandbox is not None:\n                sandbox.reset()\n                self.sandbox_pool.put(sandbox)\n    \n        mean_entropy = self._compute_mean_entropy(logprobs_buffer)\n    \n        return {\n            'Attempt': attempt_index + 1, \n            'Response Length': total_tokens, \n            'Python Calls': python_calls, \n            'Python Errors': python_errors, \n            'Entropy': mean_entropy, \n            'Answer': final_answer\n        }\n    \n    def _select_answer(self, detailed_results: list) -> int:\n\n        answer_weights = defaultdict(float)\n        answer_votes = defaultdict(int)\n\n        for result in detailed_results:\n            answer = result['Answer']\n            entropy = result['Entropy']\n            \n            if answer is not None:\n                weight = 1.0 / max(entropy, 1e-9)\n                \n                answer_weights[answer] += weight\n                answer_votes[answer] += 1\n\n        scored_answers = []\n\n        for answer, total_weight in answer_weights.items():\n            scored_answers.append({\n                'answer': answer, \n                'votes': answer_votes[answer], \n                'score': total_weight\n            })\n\n        scored_answers.sort(key=lambda x: x['score'], reverse=True)\n\n        vote_data = []\n\n        for item in scored_answers:\n            vote_data.append((\n                item['answer'], \n                item['votes'], \n                item['score']\n            ))\n\n        vote_dataframe = pd.DataFrame(\n            vote_data, \n            columns=['Answer', 'Votes', 'Score']\n        )\n\n        vote_dataframe = vote_dataframe.round({'Score': 3})\n        display(vote_dataframe)\n        \n        if not scored_answers:\n            print('\\nFinal Answer: 0\\n')\n            return 0\n\n        final_answer = scored_answers[0]['answer']    \n        print(f'\\nFinal Answer: {final_answer}\\n')\n\n        return final_answer\n    \n    def solve_problem(self, problem: str) -> int:\n    \n        print(f'\\nProblem: {problem}\\n')\n        \n        user_input = f'{problem} {self.cfg.preference_prompt}'\n    \n        elapsed_global = time.time() - self.notebook_start_time\n        time_left = self.cfg.notebook_limit - elapsed_global\n        problems_left_others = max(0, self.problems_remaining - 1)\n        reserved_time = problems_left_others * self.cfg.base_problem_timeout\n    \n        budget = time_left - reserved_time\n        budget = min(budget, self.cfg.high_problem_timeout)\n        budget = max(budget, self.cfg.base_problem_timeout)\n    \n        deadline = time.time() + budget\n    \n        print(f'Budget: {budget:.2f} seconds | Deadline: {deadline:.2f}\\n')\n    \n        tasks = []\n    \n        for attempt_index in range(self.cfg.attempts):\n            tasks.append((self.cfg.system_prompt, attempt_index))\n    \n        detailed_results = []\n        valid_answers = []\n    \n        stop_event = threading.Event()\n    \n        executor = ThreadPoolExecutor(max_workers=self.cfg.workers)\n    \n        try:\n            futures = []\n    \n            for (system_prompt, attempt_index) in tasks:\n                future = executor.submit(\n                    self._process_attempt, \n                    user_input, \n                    system_prompt, \n                    attempt_index, \n                    stop_event, \n                    deadline\n                )\n    \n                futures.append(future)\n    \n            for future in as_completed(futures):\n                try:\n                    result = future.result()\n                    detailed_results.append(result)\n    \n                    if result['Answer'] is not None:\n                        valid_answers.append(result['Answer'])\n    \n                    counts = Counter(valid_answers).most_common(1)\n    \n                    if counts and counts[0][1] >= self.cfg.early_stop:\n                        stop_event.set()\n    \n                        for f in futures:\n                            f.cancel()\n    \n                        break\n    \n                except Exception as exc:\n                    print(f'Future failed: {exc}')\n                    continue\n    \n        finally:\n            stop_event.set()\n            executor.shutdown(wait=True, cancel_futures=True)\n            \n            self.problems_remaining = max(0, self.problems_remaining - 1)\n    \n        if detailed_results:\n            results_dataframe = pd.DataFrame(detailed_results)\n            results_dataframe['Entropy'] = results_dataframe['Entropy'].round(3)\n            results_dataframe['Answer'] = results_dataframe['Answer'].astype('Int64')\n            \n            display(results_dataframe)\n    \n        if not valid_answers:\n            print('\\nResult: 0\\n')\n    \n            return 0\n    \n        return self._select_answer(detailed_results)\n    \n    def __del__(self):\n    \n        if hasattr(self, 'server_process'):\n            self.server_process.terminate()\n            self.server_process.wait()\n    \n        if hasattr(self, 'log_file'):\n            self.log_file.close()\n    \n        if hasattr(self, 'sandbox_pool'):\n            while not self.sandbox_pool.empty():\n                try:\n                    sb = self.sandbox_pool.get_nowait()\n                    sb.close()\n    \n                except Exception:\n                    pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"solver = AIMO3Solver(CFG)","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None) -> pl.DataFrame:\n    \n    id_value = id_.item(0)\n    question_text = question.item(0)\n    \n    gc.disable()\n    \n    final_answer = solver.solve_problem(question_text)\n    \n    gc.enable()\n    gc.collect()\n    \n    return pl.DataFrame({'id': id_value, 'answer': final_answer})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\n    \nelse:\n    inference_server.run_local_gateway(\n        ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)\n    )","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null}]}